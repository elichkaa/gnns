GpuFreq=control_disabled
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at logs/dDGM_google-embeddinggemma-300m_k15_gat_euclidean_poolmean/version_24/checkpoints/epoch=29-step=16980.ckpt
/pfs/data6/home/ka/ka_stud/ka_ufszm/.cache/pypoetry/virtualenvs/gnns-tZ7_1okr-py3.11/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:566: The dirpath has changed from './logs/dDGM_google-embeddinggemma-300m_k15_gat_euclidean_poolmean/version_24/checkpoints' to './logs/dDGM_google-embeddinggemma-300m_k15_gat_euclidean_poolmean/version_27/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name    | Type       | Params | Mode  | FLOPs
-------------------------------------------------------
0 | graph_f | ModuleList | 196 K  | train | 0    
1 | node_g  | ModuleList | 197 K  | train | 0    
2 | fc      | MLP        | 35.5 K | train | 0    
-------------------------------------------------------
429 K     Trainable params
0         Non-trainable params
429 K     Total params
1.719     Total estimated model params size (MB)
14        Modules in train mode
0         Modules in eval mode
0         Total Flops
Restored all states from the checkpoint at logs/dDGM_google-embeddinggemma-300m_k15_gat_euclidean_poolmean/version_24/checkpoints/epoch=29-step=16980.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
/pfs/data6/home/ka/ka_stud/ka_ufszm/.cache/pypoetry/virtualenvs/gnns-tZ7_1okr-py3.11/lib/python3.11/site-packages/pytorch_lightning/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.
/pfs/data6/home/ka/ka_stud/ka_ufszm/.cache/pypoetry/virtualenvs/gnns-tZ7_1okr-py3.11/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
`Trainer.fit` stopped: `max_epochs=30` reached.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
/pfs/data6/home/ka/ka_stud/ka_ufszm/.cache/pypoetry/virtualenvs/gnns-tZ7_1okr-py3.11/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
Traceback (most recent call last):
  File "/pfs/data6/home/ka/ka_stud/ka_ufszm/project/gnns/src/gnns/train.py", line 274, in <module>
    run_training_process(args)
  File "/pfs/data6/home/ka/ka_stud/ka_ufszm/project/gnns/src/gnns/train.py", line 178, in run_training_process
    no_graph_model = NoGraphWrapper(model)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_stud/ka_ufszm/project/gnns/src/gnns/train.py", line 160, in __init__
    self.hparams = model.hparams
    ^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_stud/ka_ufszm/.cache/pypoetry/virtualenvs/gnns-tZ7_1okr-py3.11/lib64/python3.11/site-packages/torch/nn/modules/module.py", line 2072, in __setattr__
    super().__setattr__(name, value)
AttributeError: property 'hparams' of 'run_training_process.<locals>.NoGraphWrapper' object has no setter
srun: error: uc2n561: task 0: Exited with exit code 1
